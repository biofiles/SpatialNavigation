{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cueva & Wei reproduction\n",
    "\n",
    "In this notebook we will try to reproduce the results of Cueva and Wei's 2018 paper \"Emergence of grid-like representations by training recurrent neural networks to perform spatial localization\" arXiv:1803.07770\n",
    "\n",
    "This will be done with a continuous-time RNN as described in the paper.\n",
    "\n",
    "This effort comprises the initial steps of a theoretical neuroscience project envolving neural behaviour regarding spatial localization.\n",
    "\n",
    "For questions or suggestions please contact kkohn@itba.edu.ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "Our network model consists of a set of recurrently connected units (N = 100). The dynamics of\n",
    "each unit in the network ui(t) is governed by the standard continuous-time RNN equation:\n",
    "\n",
    " $$ τ\\frac{dxi(t)}{dt} = −x_i(t) + \\sum_{j=1}^N [W^{rec}_{ij} u_j(t)]\n",
    " + \\sum_{k=1}^Nin [W^{in}_{ik} I_k(t) + b_i + ξ_i(t)] \\tag{1}$$ for i = 1, . . . , N. \n",
    "\n",
    "The activity of each unit, ui(t), is related to the activation of that unit, xi(t),\n",
    "through a nonlinearity which in this study we take to be $u_i(t) = tanh(xi(t))$. Each unit receives\n",
    "input from other units through the recurrent weight matrix Wrec and also receives external input,\n",
    "I(t), that enters the network through the weight matrix Win. Each unit has two sources of bias,\n",
    "bi which is learned and ξi(t) which represents noise intrinsic to the network and is taken to be\n",
    "Gaussian with zero mean and constant variance. The network was simulated using the Euler method\n",
    "for T = 500 timesteps of duration τ /10.\n",
    "\n",
    "To perform a 2D navigation task with the RNN, we linearly combine the firing rates of units in\n",
    "the network to estimate the current location of the animal. The responses of the two linear readout\n",
    "neurons, y1(t) and y2(t), are given by the following equation:\n",
    "$$y_j (t) = \\sum^N_{i=1} W_{ji}^{out} u_i(t) \\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # no. of units\n",
    "T = 500 # no. of timesteps\n",
    "tau = \n",
    "zi = np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input to the network\n",
    "The network inputs and outputs were inspired by simple spatial navigation tasks in 2D open environments. The task resembles dead-reckoning (sometimes referred to as path integration), which is\n",
    "ethologically relevant for many animal species (Darwin, 1873; Mittelstaedt & Mittelstaedt, 1980;\n",
    "Etienne & Jeffery, 2004; McNaughton et al., 2006). To be more specific, the inputs to the network\n",
    "were the animal’s speed and direction at each time step. Experimentally, it has been shown that the\n",
    "velocity signals exist in EC (Sargolini et al., 2006; Kropff et al., 2015; Hinman et al., 2016), and\n",
    "there is also evidence that such signals are necessary for grid formation (Winter et al., 2015a;b).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We optimized the network parameters Wrec, Win, b and Wout to minimize the squared error in equation (3) between target x- and y-coordinates from a two dimensional navigation task (performed in rectangular, hexagonal, and triangular arenas) and the network outputs generated according to equation (2).\n",
    "\n",
    "$$ E = \\frac{1}{MT N_{out}}\\sum^{M,T,N_{out}}_{m,t,j=1}\n",
    "(y_j (t, m) − y^{target}_j(t, m))^2\\tag{3}$$\n",
    "\n",
    "Parameters were updated with the *Hessian-free algorithm* (Martens & Sutskever, 2011) using minibatches of size M = 500 trials. In addition to minimizing the error function in equation (3) we regularized the input and output weights according to equation (4) and the squared firing rates of the units (referred to as metabolic cost) according to equation (5). In sum, the training aims to minimize a loss function, that consists of the error of the animal, the metabolic cost, and a penalty for large network parameters.\n",
    "\n",
    "$$R_{L2} = \\frac{1}{NN_{in}}\\sum^{N,N_{in}}_{i,j=1}\n",
    "(W^{in}_{ij})^2 +\\frac{1}{NN_{out}}\\sum^{N,N_{out}}_{i,j=1}\n",
    "(W^{out}_{ij})^2 \\tag{4}$$\n",
    "\n",
    "$$R_{FR} = \\frac{1}{NTM}\\sum^{N,T,M}_{i,t,m=1}\n",
    "u_i(t, m)^2 \\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
